{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Import Libraries and Define Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torchmetrics\n",
    "import timm\n",
    "import timm.optim\n",
    "import timm.scheduler\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# others\n",
    "import os, sys, datetime\n",
    "import albumentations\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from collections import Counter\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "from typing import Tuple, Dict, List\n",
    "from tqdm import tqdm \n",
    "from copy import deepcopy\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "# data augmentation\n",
    "def get_transforms(image_size):\n",
    "    transforms_train = albumentations.Compose([\n",
    "        albumentations.Transpose(p=0.5),\n",
    "        albumentations.VerticalFlip(p=0.5),\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.RandomBrightness(limit=0.2, p=0.75),\n",
    "        albumentations.RandomContrast(limit=0.2, p=0.75),\n",
    "        albumentations.OneOf([\n",
    "            albumentations.MotionBlur(blur_limit=5),\n",
    "            albumentations.MedianBlur(blur_limit=5),\n",
    "            albumentations.GaussianBlur(blur_limit=5),\n",
    "            albumentations.GaussNoise(var_limit=(5.0, 30.0)),\n",
    "        ], p=0.7),\n",
    "        albumentations.OneOf([\n",
    "            albumentations.OpticalDistortion(distort_limit=1.0),\n",
    "            albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n",
    "            albumentations.ElasticTransform(alpha=3),\n",
    "        ], p=0.7),\n",
    "        albumentations.CLAHE(clip_limit=4.0, p=0.7),\n",
    "        albumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n",
    "        albumentations.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85),\n",
    "        albumentations.Resize(image_size, image_size),\n",
    "        albumentations.Cutout(max_h_size=int(image_size*0.375), max_w_size=int(image_size*0.375), num_holes=1, p=0.7),\n",
    "        albumentations.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    transforms_val = albumentations.Compose([\n",
    "        albumentations.Resize(image_size, image_size),\n",
    "        albumentations.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    return transforms_train, transforms_val\n",
    "\n",
    "# 'torchvision.datasets.ImageFolder()' customized for applying data augmentation with albumentations library\n",
    "\n",
    "# make function to find classes in target directory\n",
    "def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
    "    \"\"\"Finds the class folder names in a target directory.\n",
    "\n",
    "    Assumes target directory is in standard image classification format.\n",
    "\n",
    "    Args:\n",
    "        directory (str): target directory to load classnames from.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))\n",
    "    \n",
    "    Example:\n",
    "        find_classes(\"food_images/train\")\n",
    "        >>> ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})\n",
    "    \"\"\"\n",
    "    # 1. get the class names by scanning the target directory\n",
    "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "    \n",
    "    # 2. raise an error if class names not found\n",
    "    if not classes:\n",
    "        raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n",
    "        \n",
    "    # 3. create a dictionary of index labels (computers prefer numerical rather than string labels)\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "# write a customized dataset class (inherits from torch.utils.data.Dataset)\n",
    "# 1. subclass torch.utils.data.Dataset\n",
    "class CustomizedImageFolder(Dataset):\n",
    "    # 2. initialize with a targ_dir and a transform parameter\n",
    "    def __init__(self, img_dir: str, transform) -> None:\n",
    "        # 3. create class attributes\n",
    "        # get all image paths\n",
    "        self.paths = list(pathlib.Path(img_dir).glob(\"*/*.jpg\")) # .png, .jpeg\n",
    "        # setup transforms\n",
    "        self.transform = transform\n",
    "        # create classes and class_to_idx attributes\n",
    "        self.classes, self.class_to_idx = find_classes(img_dir)\n",
    "    # 4. make function to load images\n",
    "    def load_image(self, index: int) -> Image.Image:\n",
    "        \"Opens an image via a path and returns it.\"\n",
    "        image_path = self.paths[index]\n",
    "        return Image.open(image_path)\n",
    "    # 5. overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the total number of samples.\"\n",
    "        return len(self.paths)\n",
    "    # 6. overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"Returns one sample of data, (img, label): (X, y).\"\n",
    "        # load image and label\n",
    "        img = self.load_image(index)\n",
    "        class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpg(.png, .jpeg)\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "        label = class_idx\n",
    "        # transform for applying data augmentation with albumentations library\n",
    "        img = np.array(img)\n",
    "        return self.transform(image=img)['image'], label # return (img, label): (X, y)\n",
    "\n",
    "# train\n",
    "def colorful(obj, color=\"cyan\", display_type=\"shine\"):\n",
    "    color_dict = {\"black\":\"30\", \"red\":\"31\", \"green\":\"32\", \"yellow\":\"33\", \"blue\":\"34\", \"purple\":\"35\", \"cyan\":\"36\", \"white\":\"37\"}\n",
    "    display_type_dict = {\"plain\":\"0\", \"highlight\":\"1\", \"underline\":\"4\", \"shine\":\"5\", \"inverse\":\"7\", \"invisible\":\"8\"}\n",
    "    s = str(obj)\n",
    "    color_code = color_dict.get(color, \"\")\n",
    "    display = display_type_dict.get(display_type, \"\")\n",
    "    out = '\\033[{};{}m'.format(display, color_code)+s+'\\033[0m'\n",
    "    return out\n",
    "\n",
    "class StepRunner:\n",
    "    def __init__(self, accelerator, net, loss_fn, metrics_dict=None, stage='train', optimizer=None, lr_scheduler=None):\n",
    "        self.net, self.loss_fn, self.metrics_dict, self.stage = net, loss_fn, metrics_dict, stage\n",
    "        self.optimizer, self.lr_scheduler = optimizer, lr_scheduler\n",
    "        self.accelerator = accelerator\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        features, labels = batch \n",
    "        \n",
    "        # forward\n",
    "        preds = self.net(features)\n",
    "        loss = self.loss_fn(preds, labels.long().flatten())\n",
    "\n",
    "        # backward\n",
    "        if self.optimizer is not None and self.stage == \"train\":\n",
    "            self.accelerator.backward(loss)\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "        all_preds = self.accelerator.gather(preds)\n",
    "        all_labels = self.accelerator.gather(labels)\n",
    "        all_loss = self.accelerator.gather(loss).sum()\n",
    "        \n",
    "        # losses\n",
    "        step_losses = {self.stage+\"_loss\":all_loss.item()}\n",
    "        \n",
    "        # metrics\n",
    "        step_metrics = {self.stage+\"_\"+name:metric_fn(all_preds, all_labels.long().flatten()).item()\n",
    "                        for name, metric_fn in self.metrics_dict.items()}\n",
    "        if self.optimizer is not None and self.stage == \"train\":\n",
    "            step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']\n",
    "        \n",
    "        return step_losses, step_metrics\n",
    "\n",
    "class EpochRunner:\n",
    "    def __init__(self, steprunner):\n",
    "        self.steprunner = steprunner\n",
    "        self.stage = steprunner.stage\n",
    "        self.steprunner.net.train() if self.stage == \"train\" else self.steprunner.net.eval()\n",
    "        self.accelerator = self.steprunner.accelerator\n",
    "        \n",
    "    def __call__(self, dataloader):\n",
    "        loop = tqdm(enumerate(dataloader, start=1), total=len(dataloader), file=sys.stdout, disable=not self.accelerator.is_local_main_process, ncols=100)\n",
    "\n",
    "        epoch_losses = {}\n",
    "        for step, batch in loop: \n",
    "            if self.stage == \"train\":\n",
    "                step_losses, step_metrics = self.steprunner(batch)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    step_losses, step_metrics = self.steprunner(batch)\n",
    "                    \n",
    "            step_log = dict(step_losses, **step_metrics)\n",
    "            for k, v in step_losses.items():\n",
    "                epoch_losses[k] = epoch_losses.get(k, 0.0) + v\n",
    "            \n",
    "            if step != len(dataloader):\n",
    "                loop.set_postfix(**step_log)\n",
    "            else:\n",
    "                epoch_metrics = step_metrics\n",
    "                epoch_metrics.update({self.stage+\"_\"+name:metric_fn.compute().item() \n",
    "                                 for name, metric_fn in self.steprunner.metrics_dict.items()})\n",
    "                epoch_losses = {k:v/step for k, v in epoch_losses.items()}\n",
    "                epoch_log = dict(epoch_losses, **epoch_metrics)\n",
    "                loop.set_postfix(**epoch_log)\n",
    "                for name, metric_fn in self.steprunner.metrics_dict.items():\n",
    "                    metric_fn.reset()\n",
    "        \n",
    "        return epoch_log\n",
    "\n",
    "class Model(nn.Module):\n",
    "    StepRunner, EpochRunner = StepRunner, EpochRunner\n",
    "    \n",
    "    def __init__(self, net, loss_fn, metrics_dict=None, optimizer=None, lr_scheduler=None):\n",
    "        super().__init__()\n",
    "        self.net, self.loss_fn, self.metrics_dict = net, loss_fn, nn.ModuleDict(metrics_dict) \n",
    "        self.optimizer = optimizer if optimizer is not None else torch.optim.Adam(self.net.parameters(), lr=1e-3)\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net.forward(x)\n",
    "\n",
    "    def train(self, train_data, val_data=None, epochs=30, patience=30, ckpt_path='checkpoint.pt', monitor=\"val_loss\", mode=\"min\", mixed_precision='no', callbacks=None):\n",
    "        self.__dict__.update(locals())\n",
    "        self.accelerator = Accelerator(mixed_precision=mixed_precision)\n",
    "        device = str(self.accelerator.device)\n",
    "        device_type = '🐌' if 'cpu' in device else '⚡️'\n",
    "        self.accelerator.print(colorful(\"<<<<<< \"+device_type+\" \"+device+\" is used >>>>>>\"))\n",
    "    \n",
    "        self.net, self.loss_fn, self.metrics_dict, self.optimizer, self.lr_scheduler = self.accelerator.prepare(\n",
    "            self.net, self.loss_fn, self.metrics_dict, self.optimizer, self.lr_scheduler)\n",
    "        \n",
    "        train_dataloader, val_dataloader = self.accelerator.prepare(train_data, val_data)\n",
    "        \n",
    "        self.history = {}\n",
    "        self.callbacks = self.accelerator.prepare(callbacks) if callbacks is not None else []\n",
    "        \n",
    "        if self.accelerator.is_local_main_process:\n",
    "            for callback_obj in self.callbacks:\n",
    "                callback_obj.on_fit_start(model=self)\n",
    "        \n",
    "        for epoch in range(1, epochs+1):\n",
    "            nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            self.accelerator.print('\\n'+'=========='*4+'%s'%nowtime+'=========='*4)\n",
    "            self.accelerator.print(\"Epoch {0} / {1}\".format(epoch, epochs)+\"\\n\")\n",
    "\n",
    "            # 1. train the model\n",
    "            train_step_runner = self.StepRunner(\n",
    "                net=self.net, \n",
    "                loss_fn=self.loss_fn, \n",
    "                metrics_dict=deepcopy(self.metrics_dict), \n",
    "                stage=\"train\", \n",
    "                optimizer=self.optimizer, \n",
    "                lr_scheduler=self.lr_scheduler, \n",
    "                accelerator=self.accelerator)\n",
    "            train_epoch_runner = self.EpochRunner(train_step_runner)\n",
    "            train_metrics = {'epoch':epoch}\n",
    "            train_metrics.update(train_epoch_runner(train_dataloader))\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step(epoch)\n",
    "            \n",
    "            for name, metric in train_metrics.items():\n",
    "                self.history[name] = self.history.get(name, []) + [metric]\n",
    "                \n",
    "            if self.accelerator.is_local_main_process:\n",
    "                for callback_obj in self.callbacks:\n",
    "                    callback_obj.on_train_epoch_end(model=self)\n",
    "\n",
    "            # 2. validate the model\n",
    "            if val_dataloader:\n",
    "                val_step_runner = self.StepRunner(\n",
    "                    net=self.net, \n",
    "                    loss_fn=self.loss_fn, \n",
    "                    metrics_dict=deepcopy(self.metrics_dict), \n",
    "                    stage=\"val\", \n",
    "                    accelerator=self.accelerator)\n",
    "                val_epoch_runner = self.EpochRunner(val_step_runner)\n",
    "                with torch.no_grad():\n",
    "                    val_metrics = val_epoch_runner(val_dataloader)\n",
    "\n",
    "                for name, metric in val_metrics.items():\n",
    "                    self.history[name] = self.history.get(name, []) + [metric]\n",
    "                \n",
    "                if self.accelerator.is_local_main_process:\n",
    "                    for callback_obj in self.callbacks:\n",
    "                        callback_obj.on_validation_epoch_end(model=self)\n",
    "\n",
    "            # 3. save the best model\n",
    "            self.accelerator.wait_for_everyone()\n",
    "            arr_scores = self.history[monitor]\n",
    "            best_score_idx = np.argmax(arr_scores) if mode == \"max\" else np.argmin(arr_scores)\n",
    "\n",
    "            if best_score_idx == len(arr_scores) - 1:\n",
    "                unwrapped_net = self.accelerator.unwrap_model(self.net)\n",
    "                self.accelerator.save(unwrapped_net.state_dict(), ckpt_path)\n",
    "                self.accelerator.print(colorful(\"<<<<<< reach best {0} : {1} >>>>>>\".format(monitor, arr_scores[best_score_idx])))\n",
    "\n",
    "            if len(arr_scores) - best_score_idx > patience:\n",
    "                self.accelerator.print(colorful(\"<<<<<< {} without improvement in {} epoch, early stopping >>>>>>\".format(monitor, patience)))\n",
    "                break\n",
    "\n",
    "        if self.accelerator.is_local_main_process:\n",
    "            for callback_obj in self.callbacks:\n",
    "                callback_obj.on_fit_end(model=self)\n",
    "            \n",
    "            self.net = self.accelerator.unwrap_model(self.net)\n",
    "            self.net.load_state_dict(torch.load(ckpt_path))\n",
    "            dfhistory = pd.DataFrame(self.history)\n",
    "            self.accelerator.print(dfhistory)\n",
    "            return dfhistory \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, val_data):\n",
    "        accelerator = Accelerator()\n",
    "        self.net, self.loss_fn, self.metrics_dict = accelerator.prepare(self.net, self.loss_fn, self.metrics_dict)\n",
    "        val_data = accelerator.prepare(val_data)\n",
    "        val_step_runner = self.StepRunner(\n",
    "            net=self.net, \n",
    "            loss_fn=self.loss_fn, \n",
    "            metrics_dict=deepcopy(self.metrics_dict), \n",
    "            stage=\"val\", \n",
    "            accelerator=accelerator)\n",
    "        val_epoch_runner = self.EpochRunner(val_step_runner)\n",
    "        val_metrics = val_epoch_runner(val_data)\n",
    "        return val_metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Load and Pre-Process the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 300\n",
    "num_classes = 6\n",
    "transform_augmentation, transform_normal = get_transforms(img_size)\n",
    "\n",
    "fold = '_Fold1' # Stratified 5-Fold Cross Validation\n",
    "img_train_dir = '/.../PAD-UFES-20_300x300_SoG_Split4-1-1'+str(fold)+'/train/'\n",
    "img_val_dir = '/.../PAD-UFES-20_300x300_SoG_Split4-1-1'+str(fold)+'/val/'\n",
    "img_test_dir = '/.../PAD-UFES-20_300x300_SoG_Split4-1-1'+str(fold)+'/test/'\n",
    "\n",
    "ds_train = CustomizedImageFolder(img_train_dir, transform=transform_augmentation)\n",
    "ds_val = CustomizedImageFolder(img_val_dir, transform=transform_normal)\n",
    "ds_test = CustomizedImageFolder(img_test_dir, transform=transform_normal)\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=64, shuffle=True, drop_last=True)\n",
    "dl_val = DataLoader(ds_val, batch_size=64, shuffle=False)\n",
    "dl_test = DataLoader(ds_test, batch_size=64, shuffle=False)\n",
    "\n",
    "# print('Examine Numerical Labels: ', ds_train.class_to_idx)\n",
    "# for features, labels in dl_train:\n",
    "#     # shape of features: [batch_size; channels, height, width]\n",
    "#     print('Examine Batched Data Shapes: ', features.shape, labels.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate normalized inverse class frequencies\n",
    "\n",
    "# gather all labels from the DataLoader\n",
    "all_labels = []\n",
    "for _, labels in dl_train:\n",
    "    all_labels.extend(labels.tolist())\n",
    "\n",
    "# count the instances of each class\n",
    "class_counts = Counter(all_labels)\n",
    "\n",
    "# calculate the total number of instances\n",
    "total_instances = len(all_labels)\n",
    "\n",
    "# calculate inverse class frequencies\n",
    "inverse_class_frequencies = {class_label: total_instances/count for class_label, count in class_counts.items()}\n",
    "\n",
    "# convert to tensor or list, assuming classes are 0-indexed and continuous\n",
    "num_classes = len(class_counts)\n",
    "inverse_freq_tensor = torch.zeros(num_classes)\n",
    "for class_label, freq in inverse_class_frequencies.items():\n",
    "    inverse_freq_tensor[class_label] = freq\n",
    "normalized_inverse_freq_tensor = inverse_freq_tensor / inverse_freq_tensor.sum()\n",
    "normalized_inverse_freq = normalized_inverse_freq_tensor.tolist()\n",
    "print('Normalized Inverse Class Frequencies: ', normalized_inverse_freq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Define the Model and Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNetB3\n",
    "net = timm.create_model('efficientnet_b3', features_only=False, pretrained=True, num_classes=num_classes)\n",
    "# torchkeras.summary(net, input_shape=(3, img_size, img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass accuracy\n",
    "class MulticlassAccuracy(torchmetrics.Accuracy):\n",
    "    def __init__(self, multiclass=True, num_classes=6, average='micro', dist_sync_on_step=False):\n",
    "        super().__init__(multiclass=multiclass, num_classes=num_classes, average=average, dist_sync_on_step=dist_sync_on_step)\n",
    "        \n",
    "    def update(self, preds:torch.Tensor, targets:torch.Tensor):\n",
    "        super().update(preds.argmax(dim=-1), targets.long().flatten())\n",
    "        \n",
    "    def compute(self):\n",
    "        return super().compute()\n",
    "\n",
    "# balanced multiclass accuracy\n",
    "class BalancedMulticlassAccuracy(torchmetrics.Accuracy):\n",
    "    def __init__(self, multiclass=True, num_classes=6, average='macro', dist_sync_on_step=False):\n",
    "        super().__init__(multiclass=multiclass, num_classes=num_classes, average=average, dist_sync_on_step=dist_sync_on_step)\n",
    "        \n",
    "    def update(self, preds:torch.Tensor, targets:torch.Tensor):\n",
    "        super().update(preds.argmax(dim=-1), targets.long().flatten())\n",
    "        \n",
    "    def compute(self):\n",
    "        return super().compute()\n",
    "    \n",
    "# AUROC\n",
    "class AUROC(torchmetrics.AUROC):\n",
    "    def __init__(self, num_classes=6, average='macro', dist_sync_on_step=False):\n",
    "        super().__init__(num_classes=num_classes, average=average, dist_sync_on_step=dist_sync_on_step)\n",
    "        \n",
    "    def update(self, preds:torch.Tensor, targets:torch.Tensor):\n",
    "        super().update((nn.Softmax(dim=1)(preds)), targets.long().flatten())\n",
    "        \n",
    "    def compute(self):\n",
    "        return super().compute()\n",
    "\n",
    "# focal loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)): self.alpha = torch.Tensor([alpha, 1-alpha])\n",
    "        if isinstance(alpha, list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim() > 2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)\n",
    "            input = input.transpose(1, 2)\n",
    "            input = input.contiguous().view(-1, input.size(2))\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = nn.LogSoftmax(dim=1)(input)\n",
    "        logpt = logpt.gather(1, target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = logpt.exp()\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt*at\n",
    "\n",
    "        loss = -1*(1-pt)**self.gamma*logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Train and Save the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(4317)\n",
    "\n",
    "epochs = 100\n",
    "patience = 100\n",
    "loss_fn = FocalLoss(alpha=normalized_inverse_freq)\n",
    "metrics_dict = {'BACC':BalancedMulticlassAccuracy()}\n",
    "optimizer = torch.optim.Adam(params=net.parameters(), lr=1e-4)\n",
    "lr_scheduler = timm.scheduler.CosineLRScheduler(optimizer=optimizer, t_initial=epochs, lr_min=1e-5, warmup_t=math.ceil(epochs/10), warmup_lr_init=1e-5)\n",
    "\n",
    "model = Model(net, loss_fn, metrics_dict, optimizer, lr_scheduler=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhistory = model.train(\n",
    "    train_data=dl_train, \n",
    "    val_data=dl_val, \n",
    "    epochs=epochs, \n",
    "    patience=patience, \n",
    "    ckpt_path='ImageOnlyDNN'+str(fold)+'.pt', \n",
    "    monitor='val_BACC', \n",
    "    mode='max', \n",
    "    mixed_precision='no')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Load and Evaluate the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {'ACC':MulticlassAccuracy(), 'BACC':BalancedMulticlassAccuracy(), 'AUROC':AUROC()}\n",
    "model = Model(net, loss_fn, metrics_dict, optimizer, lr_scheduler=lr_scheduler)\n",
    "model.net.load_state_dict(torch.load('/.../checkpoints/ImageOnlyDNN'+str(fold)+'.pt'))\n",
    "model.net = model.net.cuda()\n",
    "model.net.eval()\n",
    "\n",
    "# print(model.evaluate(dl_train))\n",
    "print(model.evaluate(dl_val))\n",
    "print(model.evaluate(dl_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Save Classification Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = CustomizedImageFolder(img_train_dir, transform=transform_normal)\n",
    "\n",
    "cnn_train = []\n",
    "id_train = []\n",
    "for i in range(len(ds_train)):\n",
    "    img, label = ds_train[i]\n",
    "    tensor = img.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    y_prob = (model.net(tensor[None, ...])).reshape(6).cpu().detach().numpy()\n",
    "    cnn_train.append(y_prob)\n",
    "    id_train.append(os.path.basename(ds_train.paths[i]))\n",
    "dnn_train = pd.DataFrame(data=cnn_train).to_csv('cnn_train.csv')\n",
    "id_train = pd.DataFrame(data=id_train).to_csv('id_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_val = CustomizedImageFolder(img_val_dir, transform=transform_normal)\n",
    "\n",
    "cnn_val = []\n",
    "id_val = []\n",
    "for i in range(len(ds_val)):\n",
    "    img, label = ds_val[i]\n",
    "    tensor = img.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    y_prob = (model.net(tensor[None, ...])).reshape(6).cpu().detach().numpy()\n",
    "    cnn_val.append(y_prob)\n",
    "    id_val.append(os.path.basename(ds_val.paths[i]))\n",
    "dnn_val = pd.DataFrame(data=cnn_val).to_csv('cnn_val.csv')\n",
    "id_val = pd.DataFrame(data=id_val).to_csv('id_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = CustomizedImageFolder(img_test_dir, transform=transform_normal)\n",
    "\n",
    "cnn_test = []\n",
    "id_test = []\n",
    "for i in range(len(ds_test)):\n",
    "    img, label = ds_test[i]\n",
    "    tensor = img.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    y_prob = (model.net(tensor[None, ...])).reshape(6).cpu().detach().numpy()\n",
    "    cnn_test.append(y_prob)\n",
    "    id_test.append(os.path.basename(ds_test.paths[i]))\n",
    "dnn_test = pd.DataFrame(data=cnn_test).to_csv('cnn_test.csv')\n",
    "id_test = pd.DataFrame(data=id_test).to_csv('id_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d844e3f8642a194b776b23a167f67ed705eb7f693544df137c8957dd067974a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
